{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Conversational AI Assignment 1: Fine-tuning for Knowledge-aware Response Generation\n",
    "\n",
    "LLMs are being widely deployed as dialogue systems, e.g. QA systems. When we want to ask domain-specific questions, an off-the-shelf language model might lack the desired knowledge. Adding the desired knowledge and fine-tuning can address these gaps. \n",
    "\n",
    "In this assignment, we will be fine-tuning a large language model (Llama3.2 1B Instruct) for task-oriented conversational modeling with subjective knowledge. The task and dataset are explained in detail [here](https://github.com/alexa/dstc11-track5).\n",
    "\n",
    "In essence, the task is to respond to subjective user requests regarding hotels and restaurants, e.g. \"does the restaurant have a nice vibe?\". Our conversational model has to generate a response to this question based on conversation history and previous reviews it has access to. \n",
    "\n",
    "This task consists of three sub-tasks: \n",
    "1. Knowledge-seeking Turn Detection\n",
    "2. Knowledge Selection\n",
    "3. **Knowledge-grounded Response Generation** (our focus)\n",
    "\n",
    "We will be focusing on the last sub-task, _knowledge-grounded response generation_, for which we need the dialog history, request, and knowledge. With these as the input, the model should then generate a response.\n",
    "\n",
    "**This notebook contains the following parts:**\n",
    "1. Exploring the DSTC11 Task5 data and formatting it in the proper structure\n",
    "2. Off-the-shelf inference with Llama3.2\n",
    "3. Fine-tuning Llama3.2 on the DSTC11 Task5 data\n",
    "4. Analyzing the model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "We will be using the dialogue history between a user and a system and subjective knowledge w.r.t. relevant user reviews to have Llama3.2 generate a response to a knowledge-seeking request from the user. The goal is to compare the performance between off-the-shelf usage and a domain-specific fine-tuned model. For this assignment, we will focus on manual human evaluation. \n",
    "\n",
    "Throughout the notebook, you will find questions you need to answer to complete the assignment. These are both coding questions as well as report questions for you to understand the data, as well as understanding the model behavior. These questions are indicated as  **<span style=\"background:yellow\">Q#</span>**.\n",
    "\n",
    "**Assignment steps:**\n",
    "1. Load the dataset and understand its structure, individual components, what the inputs should be and what the output should be. **(Q1)**\n",
    "2. Convert the dataset to a structure that is useful for our LLM to generate its responses with. **(Q2 - Q5)**\n",
    "3. Manually examine off-the-shelf/vanilla Llama3.2 performance on 10 dataset samples. **(Q6)**\n",
    "4. Fine-tune Llama3.2 on our dataset. **(Q7)**\n",
    "5. Manually examine finetuned Llama3.2 performance. **(Q8 - Q13)**\n",
    "6. Write a 4-pages report to describe your findings, maximum 5 pages in LNCS format. The report should consist of the following: \n",
    "    1. *Introduction* - 1-page that introduces the task, model, summary of methodology, and findings\n",
    "    2. *Data* - 1-page that describes the dataset we use:\n",
    "        1. How does the dataset look like? \n",
    "        2. What are the individual components and how are they relevant for the LLM? \n",
    "        3. What pre-processing steps do we take and why? \n",
    "    3. *Methodology* - 0.5-page that describes the process for fine-tuning: \n",
    "        1. What are the inputs and what is the target output? \n",
    "        2. Which hyperparameters (i.e. amount of epochs, learning rate, and warmup steps) did you use? \n",
    "        3. What is the training loss you achieved and what does this mean for your model (e.g. a training loss around 0 probably hints at overfitting and above 1 means that the model isn't learning the task properly).\n",
    "    4. *Results* - 1-page that describes the findings of the manual examination, based on the questions Q8-Q13 that you will find in this notebook \n",
    "    5. *Division of work* - a short paragraph describing how the steps of the coding and report writing were tackled between the two group members.\n",
    "    \n",
    "Feel free to add more information for each section if it fits within the page limit! \n",
    "\n",
    "**Submission:**\n",
    "Please submit your code (as a Kaggle notebook) and your report (PDF) on Canvas by **15th November 23:59**.\n",
    "\n",
    "**Grading:** The grading will be split into two parts: \n",
    "1. Code accuracy and quality for steps 1-5 (40% of the grade)\n",
    "2. Quality of the report (60% of the grade)\n",
    "\n",
    "## A couple of notes about Kaggle notebooks\n",
    "* [Terms and conditions](https://www.kaggle.com/terms)\n",
    "* Short note on the [GPU limits](https://www.kaggle.com/discussions/general/108481)\n",
    "* [Uploading your own model to Kaggle](https://www.kaggle.com/discussions/questions-and-answers/63328)\n",
    "\n",
    "**Kaggle directories** may be confusing because they are present on another machine that we have no access to (effectively \"in the cloud\"). \n",
    "To help your orientation, keep this in mind:\n",
    "* Input data files are available in the read-only \"/kaggle/input/\" directory\n",
    "* You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "* You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "Kaggle seems to run smoothly, including in terms of GPUs. There are two common environment issues that happen because we forget to turn on the internet or the GPUs. Specifically:\n",
    "* `ERROR: Could not find a version that satisfies the requirement ... (from versions: none) ERROR: No matching distribution found for ...` - to solve this issue, turn your notebook Internet ON.\n",
    "* `OSError` when loading the Ollama models - to solve this, make sure your accelator is set to `GPU P100` or `GPU T4x2`\n",
    "\n",
    "So, these are four good practices when working with Kaggle notebooks:\n",
    "1. Pay attention to usage statistics, especially memory, CPU, and GPU\n",
    "2. Pay attention to the quota of GPU (measured in hours)\n",
    "3. \"Turn OFF\" the internet after each session. Turn on the internet when starting a session.\n",
    "4. \"Turn off\" the accelerator after each use. Turn the accelerator on when starting a session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T12:30:13.868175Z",
     "iopub.status.busy": "2024-10-06T12:30:13.867722Z",
     "iopub.status.idle": "2024-10-06T12:30:13.88346Z",
     "shell.execute_reply": "2024-10-06T12:30:13.881936Z",
     "shell.execute_reply.started": "2024-10-06T12:30:13.868131Z"
    }
   },
   "source": [
    "## Exploring the data\n",
    "The data we will use contains these essential components: \n",
    "- __knowledge__: The reviews for the corresponding restaurant or hotel\n",
    "- __dialogue__: The dialogue history between the user and the system\n",
    "- __response__: The ground-truth response we expect from the system, which is the target response for the lanuage model we fine-tune\n",
    "\n",
    "Let's load the data first to understand it better! The data is already attached to this notebook under datasets as _dstc-11-task-5_ and we can use the original dataloader to get the correct formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:23:05.993138Z",
     "iopub.status.busy": "2024-10-14T15:23:05.992446Z",
     "iopub.status.idle": "2024-10-14T15:23:07.170475Z",
     "shell.execute_reply": "2024-10-14T15:23:07.169649Z",
     "shell.execute_reply.started": "2024-10-14T15:23:05.993073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dataset loader\n",
    "import sys\n",
    "sys.path.insert(1, \"datasetloader\")\n",
    "from dataset_walker import DatasetWalker\n",
    "\n",
    "# Load trainingset\n",
    "train_ds = DatasetWalker(\"train\", \"dataset\", labels=True, incl_knowledge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:27.789308Z",
     "iopub.status.busy": "2024-10-14T15:24:27.788336Z",
     "iopub.status.idle": "2024-10-14T15:24:27.799633Z",
     "shell.execute_reply": "2024-10-14T15:24:27.798553Z",
     "shell.execute_reply.started": "2024-10-14T15:24:27.789256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'speaker': 'U',\n",
       "   'text': 'Can you help me find a place to stay that is moderately priced and includes free wifi?'},\n",
       "  {'speaker': 'S', 'text': 'sure, i have 17 options for you'},\n",
       "  {'speaker': 'U',\n",
       "   'text': \"Are any of them in the south? I'd like free parking too.\"},\n",
       "  {'speaker': 'S',\n",
       "   'text': 'Yes, two are in the south and both have free parking and internet. I recommend the Bridge Guesthouse. Would you like me to book a reservation?'},\n",
       "  {'speaker': 'U',\n",
       "   'text': 'I have back issues. Does this place have comfortable beds?'}],\n",
       " {'target': True,\n",
       "  'knowledge': [{'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 3,\n",
       "    'sent_id': 1,\n",
       "    'sent': 'The room was clean and comfortable and not expensive.'},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 2,\n",
       "    'sent_id': 6,\n",
       "    'sent': 'It could ruin your stay if you mind that kind of thing.'},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 4,\n",
       "    'sent_id': 3,\n",
       "    'sent': \"Sadly though, I found that the bed in the room wasn't very comfortable at all.\"},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 2,\n",
       "    'sent_id': 5,\n",
       "    'sent': 'I do have to say, though, the bed is extremely uncomfortable.'},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 3,\n",
       "    'sent_id': 5,\n",
       "    'sent': 'and the interior of the room was very good and bed was also very much comfortable.'}],\n",
       "  'response': 'The Bridge Guest House is known for having pretty uncomfortable beds according to most guests. Only one guest found it to be comfortable.'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_ds[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation History\n",
    "The first index is the dialogue, clearly marking what the user and system have said until now. The last dictionary in this list is the last utterance from the dialogue, indicating the query that the model should respond to. \n",
    "\n",
    "<span style=\"background:yellow\">__Q1:__ In the block below, write the code to first access the dialogue from _sample_, and then the query:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:36.873236Z",
     "iopub.status.busy": "2024-10-14T15:24:36.872484Z",
     "iopub.status.idle": "2024-10-14T15:24:36.879948Z",
     "shell.execute_reply": "2024-10-14T15:24:36.879014Z",
     "shell.execute_reply.started": "2024-10-14T15:24:36.873197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'speaker': 'U',\n",
       "   'text': 'Can you help me find a place to stay that is moderately priced and includes free wifi?'},\n",
       "  {'speaker': 'S', 'text': 'sure, i have 17 options for you'},\n",
       "  {'speaker': 'U',\n",
       "   'text': \"Are any of them in the south? I'd like free parking too.\"},\n",
       "  {'speaker': 'S',\n",
       "   'text': 'Yes, two are in the south and both have free parking and internet. I recommend the Bridge Guesthouse. Would you like me to book a reservation?'},\n",
       "  {'speaker': 'U',\n",
       "   'text': 'I have back issues. Does this place have comfortable beds?'}],\n",
       " {'target': True,\n",
       "  'knowledge': [{'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 3,\n",
       "    'sent_id': 1,\n",
       "    'sent': 'The room was clean and comfortable and not expensive.'},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 2,\n",
       "    'sent_id': 6,\n",
       "    'sent': 'It could ruin your stay if you mind that kind of thing.'},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 4,\n",
       "    'sent_id': 3,\n",
       "    'sent': \"Sadly though, I found that the bed in the room wasn't very comfortable at all.\"},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 2,\n",
       "    'sent_id': 5,\n",
       "    'sent': 'I do have to say, though, the bed is extremely uncomfortable.'},\n",
       "   {'domain': 'hotel',\n",
       "    'entity_id': 11,\n",
       "    'doc_type': 'review',\n",
       "    'doc_id': 3,\n",
       "    'sent_id': 5,\n",
       "    'sent': 'and the interior of the room was very good and bed was also very much comfortable.'}],\n",
       "  'response': 'The Bridge Guest House is known for having pretty uncomfortable beds according to most guests. Only one guest found it to be comfortable.'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here: \n",
    "dialogue = sample[0]\n",
    "query = sample[1]\n",
    "\n",
    "dialogue, query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q2:__ To make it more readable for the model (and also ourselves), let's reformat the conversation history to the following:</span>\n",
    "\n",
    ">__User:__ Can you help me find a place to stay that is moderately priced and includes free wifi?\n",
    ">\n",
    ">__System:__ sure, i have 17 options for you \n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:37.839907Z",
     "iopub.status.busy": "2024-10-14T15:24:37.839536Z",
     "iopub.status.idle": "2024-10-14T15:24:37.846644Z",
     "shell.execute_reply": "2024-10-14T15:24:37.845621Z",
     "shell.execute_reply.started": "2024-10-14T15:24:37.83987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:Can you help me find a place to stay that is moderately priced and includes free wifi?\n",
      "System:sure, i have 17 options for you\n",
      "User:Are any of them in the south? I'd like free parking too.\n",
      "System:Yes, two are in the south and both have free parking and internet. I recommend the Bridge Guesthouse. Would you like me to book a reservation?\n",
      "User:I have back issues. Does this place have comfortable beds?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def format_dialogue(dialogue: List[dict]) -> str: \n",
    "    \"\"\"Formats a list of dialogue by turning it into readable string representation.\n",
    "\n",
    "    Given a list of dictionaries, each representing a dialogue turn, this function formats the dialogue by prefixing\n",
    "    each turn with either \"User\" or \"System\" based on the speaker. The speaker is identified by the 'speaker' key in \n",
    "    the dictionary: 'U' for user, else it is the system.\n",
    "\n",
    "    Args:\n",
    "        dialogue (List[dict]): A list of dictionaries where each dictionary contains two keys:\n",
    "            - 'speaker' (str): A string indicating the speaker of the turn ('U' for user, 'S' for system).\n",
    "            - 'text' (str): The text spoken by the respective speaker.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string where each dialogue turn is on a new line, prefixed by either \"User\" or \"System\".\n",
    "    \"\"\"\n",
    "    # Your solution here\n",
    "    output = \"\"\n",
    "    \n",
    "    for text in dialogue:\n",
    "        output += (\"User:\" if text[\"speaker\"] == \"U\" else \"System:\") + text[\"text\"] + \"\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "print(format_dialogue(dialogue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge\n",
    "\n",
    "Let's look at the knowledge, which contains the reviews for the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:38.584515Z",
     "iopub.status.busy": "2024-10-14T15:24:38.5836Z",
     "iopub.status.idle": "2024-10-14T15:24:38.59157Z",
     "shell.execute_reply": "2024-10-14T15:24:38.590666Z",
     "shell.execute_reply.started": "2024-10-14T15:24:38.584473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'domain': 'hotel',\n",
       "  'entity_id': 11,\n",
       "  'doc_type': 'review',\n",
       "  'doc_id': 3,\n",
       "  'sent_id': 1,\n",
       "  'sent': 'The room was clean and comfortable and not expensive.'},\n",
       " {'domain': 'hotel',\n",
       "  'entity_id': 11,\n",
       "  'doc_type': 'review',\n",
       "  'doc_id': 2,\n",
       "  'sent_id': 6,\n",
       "  'sent': 'It could ruin your stay if you mind that kind of thing.'},\n",
       " {'domain': 'hotel',\n",
       "  'entity_id': 11,\n",
       "  'doc_type': 'review',\n",
       "  'doc_id': 4,\n",
       "  'sent_id': 3,\n",
       "  'sent': \"Sadly though, I found that the bed in the room wasn't very comfortable at all.\"},\n",
       " {'domain': 'hotel',\n",
       "  'entity_id': 11,\n",
       "  'doc_type': 'review',\n",
       "  'doc_id': 2,\n",
       "  'sent_id': 5,\n",
       "  'sent': 'I do have to say, though, the bed is extremely uncomfortable.'},\n",
       " {'domain': 'hotel',\n",
       "  'entity_id': 11,\n",
       "  'doc_type': 'review',\n",
       "  'doc_id': 3,\n",
       "  'sent_id': 5,\n",
       "  'sent': 'and the interior of the room was very good and bed was also very much comfortable.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge = sample[1][\"knowledge\"]\n",
    "knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again a list of different reviews. If we were to give this knowledge directly to the LLM, we are giving it extra information that is not necessary for generating the response. We only need the review _itself_. \n",
    "\n",
    "<span style=\"background:yellow\">__Q3:__ In the block below, write a function that takes as input the knowledge of one sample and outputs the reviews in a list:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:39.376851Z",
     "iopub.status.busy": "2024-10-14T15:24:39.376197Z",
     "iopub.status.idle": "2024-10-14T15:24:39.384889Z",
     "shell.execute_reply": "2024-10-14T15:24:39.383939Z",
     "shell.execute_reply.started": "2024-10-14T15:24:39.376812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The room was clean and comfortable and not expensive.',\n",
       " 'It could ruin your stay if you mind that kind of thing.',\n",
       " \"Sadly though, I found that the bed in the room wasn't very comfortable at all.\",\n",
       " 'I do have to say, though, the bed is extremely uncomfortable.',\n",
       " 'and the interior of the room was very good and bed was also very much comfortable.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_reviews(knowledge: List[dict]) -> List[str]: \n",
    "    \"\"\"Extracts and returns a list of review sentences from the given knowledge data.\n",
    "\n",
    "    Given a list of dictionaries representing knowledge data, this function collects review text from each dictionary.\n",
    "    If there is no sentence in a specific review, it extracts the value for the 'answer' key. \n",
    "\n",
    "    Args:\n",
    "        knowledge (List[dict]): A list of dictionaries containing review information. Each dictionary has either:\n",
    "            - 'sent' (str): A key holding the review text.\n",
    "            - 'answer' (str): A fallback key used when 'sent' is not available.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings where each string is a review extracted from the knowledge data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    for data in knowledge:\n",
    "        if \"sent\" in data:\n",
    "            res.append(data[\"sent\"])\n",
    "        else:\n",
    "            res.append(data[\"answer\"])\n",
    "\n",
    "\n",
    "    return res\n",
    "    \n",
    "\n",
    "get_reviews(knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response (Ground-truth)\n",
    "Similarly, we can load the ground-truth response, which will be the target to fine-tune our LLM with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:40.115748Z",
     "iopub.status.busy": "2024-10-14T15:24:40.114993Z",
     "iopub.status.idle": "2024-10-14T15:24:40.121853Z",
     "shell.execute_reply": "2024-10-14T15:24:40.120826Z",
     "shell.execute_reply.started": "2024-10-14T15:24:40.115709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Bridge Guest House is known for having pretty uncomfortable beds according to most guests. Only one guest found it to be comfortable.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = sample[1][\"response\"]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "Until now we have looked at and applied our functions to only one sample. Later, when we want to fine-tune our model, we will be using Unsloth in combination with HuggingFace. It is thus useful to convert our dataset into a HuggingFace dataset. \n",
    "\n",
    "Let's first install Unsloth, which will also install the other dependencies like transformers and torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:24:40.932823Z",
     "iopub.status.busy": "2024-10-14T15:24:40.932449Z",
     "iopub.status.idle": "2024-10-14T15:27:19.337913Z",
     "shell.execute_reply": "2024-10-14T15:27:19.336798Z",
     "shell.execute_reply.started": "2024-10-14T15:24:40.932787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu118-torch240]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to c:\\users\\ahmad\\appdata\\local\\temp\\pip-install-ici5g7_9\\unsloth_aa4bf9f3681e41eb8c60b5fd822a8f9b\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 0c8c5ed81e423658ab9ae81eac5aab8d18f5d7af\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu118-torch240]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu118-torch240]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmad\\appdata\\roaming\\python\\python310\\site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu118-torch240]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git 'C:\\Users\\ahmad\\AppData\\Local\\Temp\\pip-install-ici5g7_9\\unsloth_aa4bf9f3681e41eb8c60b5fd822a8f9b'\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: xformers-0.0.27.post2+cu118-cp310-cp310-manylinux2014_x86_64.whl is not a supported wheel on this platform.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install torch==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now correctly format our entire dataset! Essentially, we want directly our nicely formatted dialogue, knowledge, and the ground-truth response. We can do this as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:27:19.340103Z",
     "iopub.status.busy": "2024-10-14T15:27:19.339776Z",
     "iopub.status.idle": "2024-10-14T15:27:21.800337Z",
     "shell.execute_reply": "2024-10-14T15:27:21.799366Z",
     "shell.execute_reply.started": "2024-10-14T15:27:19.340067Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def reformat_dataset(dataset): \n",
    "    reformatted_dataset = {\n",
    "        \"dialogue\": [],\n",
    "        \"knowledge\": [],\n",
    "        \"response\": [],\n",
    "    }\n",
    "    for sample in dataset: \n",
    "        # Filter out samples that do not require a response.\n",
    "        if sample[1][\"target\"]:\n",
    "            reformatted_dataset[\"dialogue\"].append(format_dialogue(sample[0]))\n",
    "            reformatted_dataset[\"knowledge\"].append(get_reviews(sample[1][\"knowledge\"]))\n",
    "            reformatted_dataset[\"response\"].append(sample[1][\"response\"])\n",
    "        \n",
    "    return reformatted_dataset\n",
    "\n",
    "reformatted_dataset = reformat_dataset(train_ds)\n",
    "dataset = Dataset.from_dict(reformatted_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we access the first sample, it is much more readable and we can access the different components directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:27:21.802035Z",
     "iopub.status.busy": "2024-10-14T15:27:21.801637Z",
     "iopub.status.idle": "2024-10-14T15:27:21.811849Z",
     "shell.execute_reply": "2024-10-14T15:27:21.810984Z",
     "shell.execute_reply.started": "2024-10-14T15:27:21.801989Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "### Setting up\n",
    "\n",
    "Now that we have understand our data a bit better, we can look at fine-tuning our model. We will be using HuggingFace Transformers + Unsloth for faster fine-tuning. \n",
    "\n",
    "For the large language model, we will be using __Llama3.2 1B Instruct__. Let's load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:27:21.816735Z",
     "iopub.status.busy": "2024-10-14T15:27:21.816453Z",
     "iopub.status.idle": "2024-10-14T15:27:59.551827Z",
     "shell.execute_reply": "2024-10-14T15:27:59.550798Z",
     "shell.execute_reply.started": "2024-10-14T15:27:21.816705Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset in the correct format and loaded our model, we need to create a simple prompt to fine-tune our model on. We can give the model the knowledge and dialogue and ask it to give the response: \n",
    "\n",
    "> __PROMPT__: '''__KNOWLEDGE__: {knowledge}\n",
    ">\n",
    "> __DIALOGUE__: {dialogue}\n",
    ">\n",
    "> __RESPONSE__:'''\n",
    "\n",
    "<span style=\"background:yellow\">__Q4:__ Let's create a function that takes as input one sample from the dataset, creates the prompt and tokenizes it along with the ground-truth target response. This is then added to the dataset. For this, we can use the [map()](https://huggingface.co/docs/datasets/process#map) function:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:27:59.553404Z",
     "iopub.status.busy": "2024-10-14T15:27:59.553056Z",
     "iopub.status.idle": "2024-10-14T15:28:27.349528Z",
     "shell.execute_reply": "2024-10-14T15:28:27.348559Z",
     "shell.execute_reply.started": "2024-10-14T15:27:59.553369Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample: dict, max_length=512) -> dict:\n",
    "    \"\"\"Prepares model inputs by creating the prompt in the correct format, then tokenizing the prompt and response.\n",
    "\n",
    "    Given a sample that contains knowledge, dialogue, and response, this function constructs a prompt according to the format above. \n",
    "    The function tokenizes both the prompt and the response, truncating and padding them to \n",
    "    ensure they match the maximum length. It also appends the EOS token to the response and tokenizes it separately to \n",
    "    create the labels for the model.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing:\n",
    "            - 'knowledge' (str): Background knowledge information.\n",
    "            - 'dialogue' (str): The dialogue/conversation history text.\n",
    "            - 'response' (str): The target response to be generated by the model.\n",
    "        max_length (int, optional): The maximum length for tokenized inputs. Defaults to 512.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized inputs for the model:\n",
    "            - 'input_ids' (List[int]): Tokenized prompt.\n",
    "            - 'attention_mask' (List[int]): Attention mask of input.\n",
    "            - 'labels' (List[int]): Tokenized response as labels.\n",
    "            - 'prompt' (str): The original prompt string.\n",
    "    \"\"\"\n",
    "    # Your solution here. \n",
    "    # prompt = .... \n",
    "    \n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q5:__ We would also like to use a validation and test set. For this, create a function that takes as input the split of the DSTC11 dataset and returns the dataset processed by all the steps we applied to the training dataset!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:28:27.351418Z",
     "iopub.status.busy": "2024-10-14T15:28:27.350985Z",
     "iopub.status.idle": "2024-10-14T15:28:36.452483Z",
     "shell.execute_reply": "2024-10-14T15:28:36.451519Z",
     "shell.execute_reply.started": "2024-10-14T15:28:27.351363Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_dataset_split(split: str) -> Dataset: \n",
    "    \"\"\"Loads, reformats, and processes a dataset split for model training or evaluation.\n",
    "\n",
    "    This function loads a dataset split (e.g., 'train', 'validation', 'test') using the `DatasetWalker` class, reformats\n",
    "    the dataset using `reformat_dataset`, and then preprocesses each entry by applying `preprocess_function`. The \n",
    "    processed dataset is returned in the form of a HuggingFace `Dataset` object.\n",
    "\n",
    "    Args:\n",
    "        split (str): The name of the dataset split to process\n",
    "\n",
    "    Returns:\n",
    "        dataset: A HuggingFace `Dataset` object that contains the preprocessed and reformatted data for the specified split.\n",
    "\n",
    "    \"\"\"\n",
    "    # Your solution here.\n",
    "    \n",
    "\n",
    "validation_ds = process_dataset_split(\"val\")\n",
    "test_ds = process_dataset_split(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-the-shelf usage\n",
    "\n",
    "To see what our LLM does without any fine-tuning, let's run inference directly on a couple of samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:28:36.45411Z",
     "iopub.status.busy": "2024-10-14T15:28:36.453789Z",
     "iopub.status.idle": "2024-10-14T15:28:41.512832Z",
     "shell.execute_reply": "2024-10-14T15:28:41.511875Z",
     "shell.execute_reply.started": "2024-10-14T15:28:36.454076Z"
    }
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inputs = tokenizer([dataset[0][\"prompt\"]], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "print(\"Ground-truth: \", dataset[0][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:28:41.514705Z",
     "iopub.status.busy": "2024-10-14T15:28:41.514297Z",
     "iopub.status.idle": "2024-10-14T15:28:43.760564Z",
     "shell.execute_reply": "2024-10-14T15:28:43.759537Z",
     "shell.execute_reply.started": "2024-10-14T15:28:41.514659Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer([dataset[1][\"prompt\"]], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "print(\"Ground-truth: \", dataset[1][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:28:43.762043Z",
     "iopub.status.busy": "2024-10-14T15:28:43.761724Z",
     "iopub.status.idle": "2024-10-14T15:28:46.009561Z",
     "shell.execute_reply": "2024-10-14T15:28:46.008592Z",
     "shell.execute_reply.started": "2024-10-14T15:28:43.76201Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer([dataset[2][\"prompt\"]], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "print(\"Ground-truth: \", dataset[2][\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "<span style=\"background:yellow\">__Q6:__ What do you observe in these generations? Are they relevant to the input? Is it responding to the query from the user?</span>\n",
    "> Write down answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's finetune!\n",
    "For fine-tuning, we use the supervised fine-tuning Trainer from HuggingFace. To understand the function better, you can read the documentation [here](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer).\n",
    "\n",
    "The outputs (model weights etc.) will be stored in the _\"outputs\"_ folder. \n",
    "\n",
    "A large part of fine-tuning is determining the values for the hyperparameters. Particularly: the batch size, number of epochs, learning rate, and warmup steps are important parameters to get right. \n",
    "\n",
    "To prevent our GPU from running out of memory, we will use gradient accumulation. With a training batch size of $2$ and gradient accumulation step of $4$ (how many forward and backward passes before updating the model weights), this essentially compares to a batch size of $8$. \n",
    "\n",
    "It is up to you to determine the values of the number of training epochs, learning rate, and warmup steps. \n",
    "In principle, the recommended standard amount of epochs to train is mostly $1-3$ epochs to prevent overfitting.\n",
    "\n",
    "The amount of warmup steps can range between $6-10\\%$ of the total amount of training steps, so this is tied to the amount of epochs you train with. \n",
    "\n",
    "For the learning rate, the usual values range between $1e-4$ and $5e-5$. \n",
    "\n",
    "Play around with these values! The aim is to have a training loss below $1$, around $0.5$. A training loss close to $0$ indicates overfitting. \n",
    "\n",
    "<span style=\"background:yellow\">__Q7:__ What values for the following hyperparameters did you set: number of epochs, learning rate, and warmup steps?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:28:46.01226Z",
     "iopub.status.busy": "2024-10-14T15:28:46.011921Z",
     "iopub.status.idle": "2024-10-14T15:28:46.226956Z",
     "shell.execute_reply": "2024-10-14T15:28:46.226058Z",
     "shell.execute_reply.started": "2024-10-14T15:28:46.012226Z"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "NUM_TRAIN_EPOCHS = #Fill in your value\n",
    "LEARNING_RATE = #Fill in your value\n",
    "WARMUP_STEPS = #Fill in your value\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset=validation_ds,\n",
    "    dataset_kwargs={'skip_prepare_dataset': True},\n",
    "    max_seq_length = 1024,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = WARMUP_STEPS,\n",
    "        num_train_epochs = NUM_TRAIN_EPOCHS, \n",
    "        learning_rate = LEARNING_RATE,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T15:28:46.228521Z",
     "iopub.status.busy": "2024-10-14T15:28:46.228201Z",
     "iopub.status.idle": "2024-10-14T18:33:46.207255Z",
     "shell.execute_reply": "2024-10-14T18:33:46.205894Z",
     "shell.execute_reply.started": "2024-10-14T15:28:46.228487Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's take a look at what our fine-tuned model does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:34:11.035087Z",
     "iopub.status.busy": "2024-10-14T18:34:11.034445Z",
     "iopub.status.idle": "2024-10-14T18:34:14.386523Z",
     "shell.execute_reply": "2024-10-14T18:34:14.385698Z",
     "shell.execute_reply.started": "2024-10-14T18:34:11.035049Z"
    }
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inputs = tokenizer([dataset[0][\"prompt\"]], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "print(\"Ground-truth: \", dataset[0][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:34:25.213109Z",
     "iopub.status.busy": "2024-10-14T18:34:25.212346Z",
     "iopub.status.idle": "2024-10-14T18:34:28.32913Z",
     "shell.execute_reply": "2024-10-14T18:34:28.328247Z",
     "shell.execute_reply.started": "2024-10-14T18:34:25.21307Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer([dataset[1][\"prompt\"]], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "print(\"Ground-truth: \", dataset[1][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:34:33.063776Z",
     "iopub.status.busy": "2024-10-14T18:34:33.062985Z",
     "iopub.status.idle": "2024-10-14T18:34:36.26044Z",
     "shell.execute_reply": "2024-10-14T18:34:36.259618Z",
     "shell.execute_reply.started": "2024-10-14T18:34:33.063736Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer([dataset[2][\"prompt\"]], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "print(\"Ground-truth: \", dataset[2][\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLLAMA\n",
    "\n",
    "Let's use OLLAMA to interact with our fine-tuned LLM. Just run the following blocks to install OLLAMA properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:34:40.533729Z",
     "iopub.status.busy": "2024-10-14T18:34:40.532996Z",
     "iopub.status.idle": "2024-10-14T18:35:12.696663Z",
     "shell.execute_reply": "2024-10-14T18:35:12.695597Z",
     "shell.execute_reply.started": "2024-10-14T18:34:40.53369Z"
    }
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:35:15.035719Z",
     "iopub.status.busy": "2024-10-14T18:35:15.034646Z",
     "iopub.status.idle": "2024-10-14T18:35:32.637774Z",
     "shell.execute_reply": "2024-10-14T18:35:32.636958Z",
     "shell.execute_reply.started": "2024-10-14T18:35:15.03567Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:43:03.92607Z",
     "iopub.status.busy": "2024-10-14T18:43:03.925672Z",
     "iopub.status.idle": "2024-10-14T18:43:07.547607Z",
     "shell.execute_reply": "2024-10-14T18:43:07.54657Z",
     "shell.execute_reply.started": "2024-10-14T18:43:03.92603Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3) # Wait for a few seconds for Ollama to load!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T18:59:43.395703Z",
     "iopub.status.busy": "2024-10-14T18:59:43.394762Z",
     "iopub.status.idle": "2024-10-14T19:00:29.145997Z",
     "shell.execute_reply": "2024-10-14T19:00:29.145033Z",
     "shell.execute_reply.started": "2024-10-14T18:59:43.395644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save our fine-tuned model to 8bit Q8_0 GGUF format so we can use it with OLLAMA\n",
    "if True: model.save_pretrained_gguf(\"model\", tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T19:03:02.13551Z",
     "iopub.status.busy": "2024-10-14T19:03:02.134594Z",
     "iopub.status.idle": "2024-10-14T19:03:08.593457Z",
     "shell.execute_reply": "2024-10-14T19:03:08.592664Z",
     "shell.execute_reply.started": "2024-10-14T19:03:02.135469Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "modelfile = '''# Modelfile\n",
    "FROM \"/kaggle/working/model/unsloth.Q8_0.gguf\"\n",
    "\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ .Response }}<|eot_id|>\"\"\"\n",
    "PARAMETER stop \"<|start_header_id|>\"\n",
    "PARAMETER stop \"<|end_header_id|>\"\n",
    "PARAMETER stop \"<|eot_id|>\"\n",
    "PARAMETER stop \"<|reserved_special_token\"\n",
    "PARAMETER top_p 0.5\n",
    "PARAMETER num_predict 42\n",
    "'''\n",
    "\n",
    "ollama.create(model='unsloth_model', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks are to give Ollama our prompt and get the response from our fine-tuned model! We can then also compare it to what the target output is. You can use the *get_ollama_output* function for the questions that will follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T19:00:58.795631Z",
     "iopub.status.busy": "2024-10-14T19:00:58.7949Z",
     "iopub.status.idle": "2024-10-14T19:00:59.619133Z",
     "shell.execute_reply": "2024-10-14T19:00:59.618005Z",
     "shell.execute_reply.started": "2024-10-14T19:00:58.79559Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ollama_output(prompt: str) -> str: \n",
    "    response = ollama.chat(model='unsloth_model', messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "        },\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T19:06:45.246Z",
     "iopub.status.busy": "2024-10-14T19:06:45.245141Z",
     "iopub.status.idle": "2024-10-14T19:06:46.217328Z",
     "shell.execute_reply": "2024-10-14T19:06:46.21636Z",
     "shell.execute_reply.started": "2024-10-14T19:06:45.245958Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 900\n",
    "print(\"INPUT:\")\n",
    "print(test_ds[idx][\"prompt\"])\n",
    "print(\"TARGET OUTPUT:\")\n",
    "print(test_ds[idx][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T19:07:03.553942Z",
     "iopub.status.busy": "2024-10-14T19:07:03.553585Z",
     "iopub.status.idle": "2024-10-14T19:07:04.743093Z",
     "shell.execute_reply": "2024-10-14T19:07:04.742336Z",
     "shell.execute_reply.started": "2024-10-14T19:07:03.553909Z"
    }
   },
   "outputs": [],
   "source": [
    "print(get_ollama_output(test_ds[idx][\"prompt\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model Responses\n",
    "Now that we have OLLAMA up and running, let's look at what types of generations the fine-tuned model produces. For this, we will be using samples from the test set! Pick 10 test samples for the analysis. This analysis will go in the report and you will use the questions below as a guide; **answer all these questions in the report**. Remember to give examples in the report when describing your findings!\n",
    "\n",
    "#### Questions\n",
    "<span style=\"background:yellow\">__Q8:__ What do you notice in terms of relevancy of the generation? Are the generations responding to the query?</span> \n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```\n",
    "\n",
    "<span style=\"background:yellow\">__Q9:__ In general, does fine-tuning improve or hurt the model's performance?</span> \n",
    "\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```\n",
    "\n",
    "<span style=\"background:yellow\">__Q10:__ How does the fine-tuned model perform in the following scenario?</span> \n",
    "1. When all reviews agree with each other\n",
    "2. When only one review disagrees\n",
    "3. When opinions in the reviews are mixed (i.e. high disagreement)\n",
    "\n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```\n",
    "\n",
    "\n",
    "<span style=\"background:yellow\">__Q11:__ How does the length of the dialogue/conversation history affect the fine-tuned model's generation?</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your coding solution and answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q12:__ Create some samples (~5) that come from a different domain (e.g. airports, shops, web-stores; you can use an LLM). How does the fine-tuned LLM perform on these?</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your coding solution and answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background:yellow\">__Q13:__ How do the responses from the fine-tuned model differ from the off-the-shelf model? Answer this for Q8-Q12</span> \n",
    "\n",
    "```\n",
    "# Your answer here\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5797867,
     "sourceId": 9522257,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5825763,
     "sourceId": 9560210,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
